# üêù BEE: Multi-Domain Autonomous Drone Project

The **BEE Project** is a modular research platform for developing and testing autonomous UAV capabilities in simulation and embedded hardware. It integrates multiple subsystems under one repository, enabling a complete workflow from **low-level flight control** to **high-level perception and mapping**.

BEE is designed as a flexible sandbox for experimenting with new ideas in **autonomous navigation, perception, and communication pipelines**.

---

## ‚úàÔ∏è Core Components

1. **Flight Controller Development (Simulation)**
    
    Custom controllers for UAV stabilization and navigation, tested in simulation with Gazebo/ROS 2. Includes PID-based stabilization, teleoperation scripts, and prototype autonomous control pipelines.
    
2. **Monocular Vision Depth Perception (Depth Anything V2)**
    
    A deep-learning-based pipeline for monocular depth estimation using *Depth Anything V2*. Generates dense depth maps for 3D scene reconstruction, odometry, and SLAM integration.
      
3. **Structure-from-Motion (SfM) & Visual Odometry**
    
    Multiple pipelines for reconstructing 3D structure from images, including ray-based triangulation, feature-based SfM, and temporal visual odometry. Provides research flexibility for comparing algorithms.
    
---

## üìÇ Repository Structure (Highlights)

- **`scripts/`** ‚Äì UAV control logic, perception pipelines, SfM experiments.
- **`launch/`** ‚Äì ROS 2 launch files for BEE simulation, RViz, and SfM scanning.
- **`CameraWebServer_OV5640AF/`** ‚Äì ESP32-CAM firmware for video streaming.
- **`Depth-Anything-V2/`** ‚Äì Monocular depth estimation pipelines.
- **`ray based approach/` & `visual odometry/`** ‚Äì Research pipelines for SfM & VO.
- **`urdf/`, `meshes/`, `worlds/`** ‚Äì UAV models and Gazebo simulation environments.

---

## üöÄ Usage Workflow

1. **Simulate the Drone** ‚Äì Launch Gazebo worlds with the BEE URDF and control nodes.
2. **Enable Flight Control** ‚Äì Run stabilization/teleop scripts for manual or assisted flight.
3. **Add Perception** ‚Äì Launch the Depth Anything V2 pipeline for depth-aware perception.
4. **Transmit Data** ‚Äì Use the ESP32-CAM telemetry stack for live video and state streaming.
5. **Experiment with SfM** ‚Äì Run visual odometry and structure-from-motion pipelines for 3D reconstruction.

---

## üìö Documentation Roadmap

- Flight Controller Development
- Monocular Depth Perception
- Structure-from-Motion Pipelines
